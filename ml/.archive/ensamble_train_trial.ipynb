{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/sjinich/disco/che_env/lib/python3.8/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "2024-03-11 16:33:53,841\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-03-11 16:33:54,071\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from engine.ensamble_model_train import deepscreen_ensamble\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>bioactivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL104783</td>\n",
       "      <td>COc1ccc(CN[C@@H](C(=O)N[C@H](C(=O)NCc2ccccc2)C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL104966</td>\n",
       "      <td>CC(C)[C@H](NC(=O)[C@H](NCc1ccccc1)[C@H](O)[C@H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL1076901</td>\n",
       "      <td>CC(C)[C@H](NC(=O)[C@H](C)C[C@H](O)[C@H](COCc1c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1076902</td>\n",
       "      <td>CC(C)[C@H](NC(=O)[C@H](C)C[C@H](O)[C@H](COCc1c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL1076905</td>\n",
       "      <td>CC(C)[C@H](NC(=O)[C@H](C)C[C@H](O)[C@H](COc1cc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>CHEMBL96943</td>\n",
       "      <td>C=CC1C=CC=CC1/C=C/OCC(=O)NC(C(=O)NC(Cc1ccccc1)...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>CHEMBL97013</td>\n",
       "      <td>CCCCNC(=O)CC(O)C(CC(C)C)NC(=O)C(NC(=O)Cc1ccc2c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2267</th>\n",
       "      <td>CHEMBL97072</td>\n",
       "      <td>C=Cc1ccccc1/C=C/OCC(=O)NC(C(=O)NC(CC(C)C)C(O)C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>CHEMBL97805</td>\n",
       "      <td>CCCCNC(=O)CC(O)C(CC(C)C)NC(=O)C(NC(=O)Cc1cc(OC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269</th>\n",
       "      <td>CHEMBL98384</td>\n",
       "      <td>CCCCNC(=O)C[C@H](O)[C@H](Cc1ccccc1)NC(=O)[C@@H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2270 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            comp_id                                             smiles  \\\n",
       "0      CHEMBL104783  COc1ccc(CN[C@@H](C(=O)N[C@H](C(=O)NCc2ccccc2)C...   \n",
       "1      CHEMBL104966  CC(C)[C@H](NC(=O)[C@H](NCc1ccccc1)[C@H](O)[C@H...   \n",
       "2     CHEMBL1076901  CC(C)[C@H](NC(=O)[C@H](C)C[C@H](O)[C@H](COCc1c...   \n",
       "3     CHEMBL1076902  CC(C)[C@H](NC(=O)[C@H](C)C[C@H](O)[C@H](COCc1c...   \n",
       "4     CHEMBL1076905  CC(C)[C@H](NC(=O)[C@H](C)C[C@H](O)[C@H](COc1cc...   \n",
       "...             ...                                                ...   \n",
       "2265    CHEMBL96943  C=CC1C=CC=CC1/C=C/OCC(=O)NC(C(=O)NC(Cc1ccccc1)...   \n",
       "2266    CHEMBL97013  CCCCNC(=O)CC(O)C(CC(C)C)NC(=O)C(NC(=O)Cc1ccc2c...   \n",
       "2267    CHEMBL97072  C=Cc1ccccc1/C=C/OCC(=O)NC(C(=O)NC(CC(C)C)C(O)C...   \n",
       "2268    CHEMBL97805  CCCCNC(=O)CC(O)C(CC(C)C)NC(=O)C(NC(=O)Cc1cc(OC...   \n",
       "2269    CHEMBL98384  CCCCNC(=O)C[C@H](O)[C@H](Cc1ccccc1)NC(=O)[C@@H...   \n",
       "\n",
       "      bioactivity  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "...           ...  \n",
       "2265            1  \n",
       "2266            1  \n",
       "2267            1  \n",
       "2268            1  \n",
       "2269            1  \n",
       "\n",
       "[2270 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antibioticos = pd.read_csv(\"/home/sjinich/disco/TrypanoDEEPscreen/.data/processed/CHEMBL2581.csv\")\n",
    "antibioticos = antibioticos[[\"comp_id\",\"smiles\",\"bioactivity\"]]\n",
    "antibioticos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    }
   ],
   "source": [
    "hpams = {\n",
    "    \"fully_layer_1\": 32, \"fully_layer_2\": 256, \"drop_rate\": 0.3, \"learning_rate\": 0.001, \"batch_size\": 32\n",
    "}\n",
    "ensamble = deepscreen_ensamble(\"../../.experiments/\",\"antibioticos_ensamble\",hpams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using hyperparameters [('fully_layer_1', 32), ('fully_layer_2', 256), ('drop_rate', 0.3), ('learning_rate', 0.001), ('batch_size', 32), ('experiment_result_path', '/big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble'), ('target', 'antibioticos_ensamble')]\n",
      "INFO: trining ensamble model with dataset train: 1816|844/972 - validation: 454|212/242\n",
      "INFO: Using a total of 2270 datapoints\n",
      "INFO: non_random_split dataset splited train=1816\n",
      "INFO: non_random_split dataset splited validation=454\n",
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble exists and is not empty.\n",
      "\n",
      "   | Name          | Type             | Params\n",
      "----------------------------------------------------\n",
      "0  | conv1         | Conv2d           | 416   \n",
      "1  | bn1           | BatchNorm2d      | 64    \n",
      "2  | conv2         | Conv2d           | 8.3 K \n",
      "3  | bn2           | BatchNorm2d      | 128   \n",
      "4  | conv3         | Conv2d           | 32.9 K\n",
      "5  | bn3           | BatchNorm2d      | 256   \n",
      "6  | conv4         | Conv2d           | 32.8 K\n",
      "7  | bn4           | BatchNorm2d      | 128   \n",
      "8  | conv5         | Conv2d           | 8.2 K \n",
      "9  | bn5           | BatchNorm2d      | 64    \n",
      "10 | pool          | MaxPool2d        | 0     \n",
      "11 | fc1           | Linear           | 25.6 K\n",
      "12 | fc2           | Linear           | 8.4 K \n",
      "13 | fc3           | Linear           | 514   \n",
      "14 | train_metrics | MetricCollection | 0     \n",
      "15 | val_metrics   | MetricCollection | 0     \n",
      "16 | test_metrics  | MetricCollection | 0     \n",
      "----------------------------------------------------\n",
      "117 K     Trainable params\n",
      "0         Non-trainable params\n",
      "117 K     Total params\n",
      "0.471     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  40%|████      | 23/57 [00:12<00:18,  1.79it/s, v_num=610, train_loss=0.957]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('train_mcc', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 57/57 [00:32<00:00,  1.78it/s, v_num=610, train_loss=0.762]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('val_mcc', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 57/57 [00:35<00:00,  1.62it/s, v_num=610, train_loss=0.677, val_loss=0.691]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 57/57 [00:35<00:00,  1.62it/s, v_num=610, train_loss=0.677, val_loss=0.691]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Ensamble model trained 0/5 - path /big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble/epoch=2-step=171-v3.ckpt\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using hyperparameters [('fully_layer_1', 32), ('fully_layer_2', 256), ('drop_rate', 0.3), ('learning_rate', 0.001), ('batch_size', 32), ('experiment_result_path', '/big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble'), ('target', 'antibioticos_ensamble')]\n",
      "INFO: trining ensamble model with dataset train: 1816|844/972 - validation: 454|212/242\n",
      "INFO: Using a total of 2270 datapoints\n",
      "INFO: non_random_split dataset splited train=1816\n",
      "INFO: non_random_split dataset splited validation=454\n",
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble exists and is not empty.\n",
      "\n",
      "   | Name          | Type             | Params\n",
      "----------------------------------------------------\n",
      "0  | conv1         | Conv2d           | 416   \n",
      "1  | bn1           | BatchNorm2d      | 64    \n",
      "2  | conv2         | Conv2d           | 8.3 K \n",
      "3  | bn2           | BatchNorm2d      | 128   \n",
      "4  | conv3         | Conv2d           | 32.9 K\n",
      "5  | bn3           | BatchNorm2d      | 256   \n",
      "6  | conv4         | Conv2d           | 32.8 K\n",
      "7  | bn4           | BatchNorm2d      | 128   \n",
      "8  | conv5         | Conv2d           | 8.2 K \n",
      "9  | bn5           | BatchNorm2d      | 64    \n",
      "10 | pool          | MaxPool2d        | 0     \n",
      "11 | fc1           | Linear           | 25.6 K\n",
      "12 | fc2           | Linear           | 8.4 K \n",
      "13 | fc3           | Linear           | 514   \n",
      "14 | train_metrics | MetricCollection | 0     \n",
      "15 | val_metrics   | MetricCollection | 0     \n",
      "16 | test_metrics  | MetricCollection | 0     \n",
      "----------------------------------------------------\n",
      "117 K     Trainable params\n",
      "0         Non-trainable params\n",
      "117 K     Total params\n",
      "0.471     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 57/57 [00:34<00:00,  1.63it/s, v_num=611, train_loss=0.655, val_loss=0.689]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 57/57 [00:34<00:00,  1.63it/s, v_num=611, train_loss=0.655, val_loss=0.689]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Ensamble model trained 1/5 - path /big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble/epoch=0-step=57-v1.ckpt\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using hyperparameters [('fully_layer_1', 32), ('fully_layer_2', 256), ('drop_rate', 0.3), ('learning_rate', 0.001), ('batch_size', 32), ('experiment_result_path', '/big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble'), ('target', 'antibioticos_ensamble')]\n",
      "INFO: trining ensamble model with dataset train: 1816|848/968 - validation: 454|208/246\n",
      "INFO: Using a total of 2270 datapoints\n",
      "INFO: non_random_split dataset splited train=1816\n",
      "INFO: non_random_split dataset splited validation=454\n",
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble exists and is not empty.\n",
      "\n",
      "   | Name          | Type             | Params\n",
      "----------------------------------------------------\n",
      "0  | conv1         | Conv2d           | 416   \n",
      "1  | bn1           | BatchNorm2d      | 64    \n",
      "2  | conv2         | Conv2d           | 8.3 K \n",
      "3  | bn2           | BatchNorm2d      | 128   \n",
      "4  | conv3         | Conv2d           | 32.9 K\n",
      "5  | bn3           | BatchNorm2d      | 256   \n",
      "6  | conv4         | Conv2d           | 32.8 K\n",
      "7  | bn4           | BatchNorm2d      | 128   \n",
      "8  | conv5         | Conv2d           | 8.2 K \n",
      "9  | bn5           | BatchNorm2d      | 64    \n",
      "10 | pool          | MaxPool2d        | 0     \n",
      "11 | fc1           | Linear           | 25.6 K\n",
      "12 | fc2           | Linear           | 8.4 K \n",
      "13 | fc3           | Linear           | 514   \n",
      "14 | train_metrics | MetricCollection | 0     \n",
      "15 | val_metrics   | MetricCollection | 0     \n",
      "16 | test_metrics  | MetricCollection | 0     \n",
      "----------------------------------------------------\n",
      "117 K     Trainable params\n",
      "0         Non-trainable params\n",
      "117 K     Total params\n",
      "0.471     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 57/57 [00:35<00:00,  1.63it/s, v_num=612, train_loss=0.677, val_loss=0.691]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 57/57 [00:35<00:00,  1.63it/s, v_num=612, train_loss=0.677, val_loss=0.691]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Ensamble model trained 2/5 - path /big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble/epoch=2-step=171-v4.ckpt\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using hyperparameters [('fully_layer_1', 32), ('fully_layer_2', 256), ('drop_rate', 0.3), ('learning_rate', 0.001), ('batch_size', 32), ('experiment_result_path', '/big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble'), ('target', 'antibioticos_ensamble')]\n",
      "INFO: trining ensamble model with dataset train: 1816|849/967 - validation: 454|207/247\n",
      "INFO: Using a total of 2270 datapoints\n",
      "INFO: non_random_split dataset splited train=1816\n",
      "INFO: non_random_split dataset splited validation=454\n",
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble exists and is not empty.\n",
      "\n",
      "   | Name          | Type             | Params\n",
      "----------------------------------------------------\n",
      "0  | conv1         | Conv2d           | 416   \n",
      "1  | bn1           | BatchNorm2d      | 64    \n",
      "2  | conv2         | Conv2d           | 8.3 K \n",
      "3  | bn2           | BatchNorm2d      | 128   \n",
      "4  | conv3         | Conv2d           | 32.9 K\n",
      "5  | bn3           | BatchNorm2d      | 256   \n",
      "6  | conv4         | Conv2d           | 32.8 K\n",
      "7  | bn4           | BatchNorm2d      | 128   \n",
      "8  | conv5         | Conv2d           | 8.2 K \n",
      "9  | bn5           | BatchNorm2d      | 64    \n",
      "10 | pool          | MaxPool2d        | 0     \n",
      "11 | fc1           | Linear           | 25.6 K\n",
      "12 | fc2           | Linear           | 8.4 K \n",
      "13 | fc3           | Linear           | 514   \n",
      "14 | train_metrics | MetricCollection | 0     \n",
      "15 | val_metrics   | MetricCollection | 0     \n",
      "16 | test_metrics  | MetricCollection | 0     \n",
      "----------------------------------------------------\n",
      "117 K     Trainable params\n",
      "0         Non-trainable params\n",
      "117 K     Total params\n",
      "0.471     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 57/57 [00:34<00:00,  1.65it/s, v_num=613, train_loss=0.698, val_loss=0.693]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 57/57 [00:34<00:00,  1.65it/s, v_num=613, train_loss=0.698, val_loss=0.693]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Ensamble model trained 3/5 - path /big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble/epoch=4-step=285-v1.ckpt\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using hyperparameters [('fully_layer_1', 32), ('fully_layer_2', 256), ('drop_rate', 0.3), ('learning_rate', 0.001), ('batch_size', 32), ('experiment_result_path', '/big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble'), ('target', 'antibioticos_ensamble')]\n",
      "INFO: trining ensamble model with dataset train: 1816|844/972 - validation: 454|212/242\n",
      "INFO: Using a total of 2270 datapoints\n",
      "INFO: non_random_split dataset splited validation=454\n",
      "INFO: non_random_split dataset splited train=1816\n",
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble exists and is not empty.\n",
      "\n",
      "   | Name          | Type             | Params\n",
      "----------------------------------------------------\n",
      "0  | conv1         | Conv2d           | 416   \n",
      "1  | bn1           | BatchNorm2d      | 64    \n",
      "2  | conv2         | Conv2d           | 8.3 K \n",
      "3  | bn2           | BatchNorm2d      | 128   \n",
      "4  | conv3         | Conv2d           | 32.9 K\n",
      "5  | bn3           | BatchNorm2d      | 256   \n",
      "6  | conv4         | Conv2d           | 32.8 K\n",
      "7  | bn4           | BatchNorm2d      | 128   \n",
      "8  | conv5         | Conv2d           | 8.2 K \n",
      "9  | bn5           | BatchNorm2d      | 64    \n",
      "10 | pool          | MaxPool2d        | 0     \n",
      "11 | fc1           | Linear           | 25.6 K\n",
      "12 | fc2           | Linear           | 8.4 K \n",
      "13 | fc3           | Linear           | 514   \n",
      "14 | train_metrics | MetricCollection | 0     \n",
      "15 | val_metrics   | MetricCollection | 0     \n",
      "16 | test_metrics  | MetricCollection | 0     \n",
      "----------------------------------------------------\n",
      "117 K     Trainable params\n",
      "0         Non-trainable params\n",
      "117 K     Total params\n",
      "0.471     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 57/57 [00:34<00:00,  1.66it/s, v_num=614, train_loss=0.663, val_loss=0.690]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 57/57 [00:34<00:00,  1.66it/s, v_num=614, train_loss=0.663, val_loss=0.690]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Ensamble model trained 4/5 - path /big/lab/sjinich/TrypanoDEEPscreen/.experiments/antibioticos_ensamble/ensamble/epoch=1-step=114-v1.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'deepscreen_ensamble' object has no attribute 'trainter_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mensamble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mantibioticos\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_mcc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/big/lab/sjinich/TrypanoDEEPscreen/src/ml/engine/ensamble_model_train.py:55\u001b[0m, in \u001b[0;36mdeepscreen_ensamble.fit\u001b[0;34m(self, data, number_to_ensamble, max_epochs, metric_to_optimize, optimize_mode)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels_ckpt_path\u001b[38;5;241m.\u001b[39mappend(checkpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path)\n\u001b[1;32m     53\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsamble model trained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(datasets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainter_models\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'deepscreen_ensamble' object has no attribute 'trainter_models'"
     ]
    }
   ],
   "source": [
    "ensamble.fit(antibioticos,5,5,\"val_mcc\",\"max\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

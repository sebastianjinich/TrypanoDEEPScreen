{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-23 17:32:20,278\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-02-23 17:32:20,441\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-02-23 17:32:22,194\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from engine.hyperparameters_tune_raytune import deepscreen_hyperparameter_tuneing\n",
    "from utils.configurations import configs\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-02-23 17:59:54</td></tr>\n",
       "<tr><td>Running for: </td><td>00:27:28.79        </td></tr>\n",
       "<tr><td>Memory:      </td><td>18.3/503.8 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=8<br>Bracket: Iter 6.000: 0.39697661002477014 | Iter 2.000: 0.3849924504756928<br>Logical resource usage: 2.0/48 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">   train_loop_config/ba\n",
       "tch_size</th><th style=\"text-align: right;\">    train_loop_config/dr\n",
       "op_rate</th><th style=\"text-align: right;\">    train_loop_config/fu\n",
       "lly_layer_1</th><th style=\"text-align: right;\">    train_loop_config/fu\n",
       "lly_layer_2</th><th style=\"text-align: right;\">       train_loop_config/le\n",
       "arning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  val_acc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_823bd_00000</td><td>TERMINATED</td><td>10.1.103.91:1319471</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">512</td><td style=\"text-align: right;\">0.005 </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">        1165.19 </td><td style=\"text-align: right;\">    0.515623</td><td style=\"text-align: right;\"> 11.3551  </td><td style=\"text-align: right;\"> 0.376374</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00001</td><td>TERMINATED</td><td>10.1.103.91:1319472</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        1644.16 </td><td style=\"text-align: right;\">    0.424775</td><td style=\"text-align: right;\">  0.404472</td><td style=\"text-align: right;\"> 0.832418</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00002</td><td>TERMINATED</td><td>10.1.103.91:1319473</td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         538.451</td><td style=\"text-align: right;\">    0.454345</td><td style=\"text-align: right;\">  0.637153</td><td style=\"text-align: right;\"> 0.604396</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00003</td><td>TERMINATED</td><td>10.1.103.91:1319474</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\">512</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         517.126</td><td style=\"text-align: right;\">    0.863634</td><td style=\"text-align: right;\">  1.07077 </td><td style=\"text-align: right;\"> 0.626374</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00004</td><td>TERMINATED</td><td>10.1.103.91:1319475</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">512</td><td style=\"text-align: right;\">0.005 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         510.685</td><td style=\"text-align: right;\">    1.42018 </td><td style=\"text-align: right;\">  0.681402</td><td style=\"text-align: right;\"> 0.541209</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00005</td><td>TERMINATED</td><td>10.1.103.91:1319476</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         503.57 </td><td style=\"text-align: right;\">    0.418373</td><td style=\"text-align: right;\">  1.81249 </td><td style=\"text-align: right;\"> 0.376374</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00006</td><td>TERMINATED</td><td>10.1.103.91:1319479</td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">0.2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">0.005 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         536.264</td><td style=\"text-align: right;\">    0.535115</td><td style=\"text-align: right;\">  1.64094 </td><td style=\"text-align: right;\"> 0.436813</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00007</td><td>TERMINATED</td><td>10.1.103.91:1319478</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.5</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        1570.09 </td><td style=\"text-align: right;\">    0.233452</td><td style=\"text-align: right;\">  0.463577</td><td style=\"text-align: right;\"> 0.815934</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00008</td><td>TERMINATED</td><td>10.1.103.91:1319477</td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">512</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         535.656</td><td style=\"text-align: right;\">    0.419671</td><td style=\"text-align: right;\">  1.89361 </td><td style=\"text-align: right;\"> 0.379121</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00009</td><td>TERMINATED</td><td>10.1.103.91:1319480</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.5</td><td style=\"text-align: right;\">512</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">0.005 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         516.41 </td><td style=\"text-align: right;\">    0.623318</td><td style=\"text-align: right;\">  0.627537</td><td style=\"text-align: right;\"> 0.651099</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=1319480)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=1319480)\u001b[0m - (ip=10.1.103.91, pid=1320314) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 512), ('drop_rate', 0.3), ('learning_rate', 0.005), ('batch_size', 32), ('experiment_result_path', '/big/lab/sjinich/TrypanoDEEPscreen/.experiments/chembl2581_trial')]\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m [rank: 0] Seed set to 123\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m Missing logger folder: /home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00000_0_batch_size=32,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0050_2024-02-23_17-32-25/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m INFO: Using a total of 2270 datapoints\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m INFO: non_random_split datasets splited train=1452,validation=364,test=454\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(RayTrainWorker pid=1320314)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(TorchTrainer pid=1319472)\u001b[0m Started distributed worker processes: \u001b[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=1319472)\u001b[0m - (ip=10.1.103.91, pid=1320421) world_rank=0, local_rank=0, node_rank=0\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m INFO: Using hyperparameters [('fully_layer_1', 256), ('fully_layer_2', 32), ('drop_rate', 0.3), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '/big/lab/sjinich/TrypanoDEEPscreen/.experiments/chembl2581_trial')]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m GPU available: False, used: False\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m TPU available: False, using: 0 TPU cores\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m IPU available: False, using: 0 IPUs\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m HPU available: False, using: 0 HPUs\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m [rank: 0] Seed set to 123\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Missing logger folder: /home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/lightning_logs\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m INFO: Using a total of 2270 datapoints\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m INFO: non_random_split datasets splited train=1452,validation=364,test=454\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320330)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('val_mcc', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "\u001b[36m(RayTrainWorker pid=1320383)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320314)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00009_9_batch_size=32,drop_rate=0.5000,fully_layer_1=512,fully_layer_2=256,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=1320402)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00008_8_batch_size=64,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000000)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00005_5_batch_size=32,drop_rate=0.2000,fully_layer_1=16,fully_layer_2=32,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('val_mcc', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "\u001b[36m(RayTrainWorker pid=1320314)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00009_9_batch_size=32,drop_rate=0.5000,fully_layer_1=512,fully_layer_2=256,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320402)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00008_8_batch_size=64,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000001)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00004_4_batch_size=32,drop_rate=0.6000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320314)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00009_9_batch_size=32,drop_rate=0.5000,fully_layer_1=512,fully_layer_2=256,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000002)\n",
      "\u001b[36m(RayTrainWorker pid=1320330)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00003_3_batch_size=32,drop_rate=0.3000,fully_layer_1=512,fully_layer_2=16,learning_rate=0.0005_2024-02-23_17-32-25/checkpoint_000002)\n",
      "\u001b[36m(RayTrainWorker pid=1320402)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00008_8_batch_size=64,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000003)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000003)\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00000_0_batch_size=32,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000003)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('train_mcc', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000004)\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('train_mcc', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000004)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000005)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('train_mcc', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000005)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000006)\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00000_0_batch_size=32,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000006)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000007)\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000007)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000008)\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000008)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m `Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000009)\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000009)\n",
      "2024-02-23 17:59:54,143\tINFO tune.py:1042 -- Total run time: 1648.85 seconds (1648.78 seconds for the tuning loop).\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m `Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*Consider increasing the value of the `num_workers` argumen*\")\n",
    "\n",
    "data = pd.read_csv(\"/home/sjinich/disco/TrypanoDEEPscreen/.data/processed/CHEMBL2581.csv\")\n",
    "    \n",
    "search_space_deepscreen = configs.get_hyperparameters_search()\n",
    "\n",
    "tuner = deepscreen_hyperparameter_tuneing(\n",
    "    data=data,\n",
    "    data_split_mode=\"non_random_split\",\n",
    "    search_space=search_space_deepscreen,\n",
    "    target=\"chembl2581_trial\",\n",
    "    experiments_result_path=\"../../.experiments\",\n",
    "    **configs.get_hyperparameters_search_setup()\n",
    "    )\n",
    "\n",
    "result = tuner.tune_deepscreen()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(result.get_best_result(metric=\"val_mcc\",mode=\"max\").checkpoint.path,\"checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 256), ('fully_layer_2', 32), ('drop_rate', 0.3), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581')]\n"
     ]
    }
   ],
   "source": [
    "from engine.system import DEEPScreenClassifier\n",
    "from datasets.datamodule import DEEPscreenDataModule\n",
    "\n",
    "model = DEEPScreenClassifier.load_from_checkpoint(ckpt_dir,experiment_result_path=\"../../.experiments/chembl2581\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../.data/processed/CHEMBL2581.csv\")\n",
    "datamodule = DEEPscreenDataModule(data=data,target_id=\"CHEMBL2581\",batch_size=32,experiment_result_path=\"../../.experiments/chembl2581\",data_split_mode=\"non_random_split\",tmp_imgs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 2270 datapoints\n",
      "INFO: non_random_split datasets splited train=1452,validation=364,test=454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  60%|██████    | 9/15 [00:02<00:01,  3.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('test_mcc', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 15/15 [00:04<00:00,  3.59it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.7819383144378662\n",
      "         test_f1            0.8176795840263367\n",
      "        test_loss           0.4834350347518921\n",
      "        test_mcc             0.555237352848053\n",
      "        test_prec           0.7708333134651184\n",
      "       test_recall          0.8705882430076599\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.4834350347518921,\n",
       "  'test_acc': 0.7819383144378662,\n",
       "  'test_f1': 0.8176795840263367,\n",
       "  'test_mcc': 0.555237352848053,\n",
       "  'test_prec': 0.7708333134651184,\n",
       "  'test_recall': 0.8705882430076599}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning import Trainer\n",
    "\n",
    "trainer = Trainer()\n",
    "\n",
    "trainer.test(model,datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_loss                                           0.454345\n",
       "val_loss                                             0.637153\n",
       "val_acc                                              0.604396\n",
       "val_f1                                               0.566265\n",
       "val_mcc                                              0.356962\n",
       "val_prec                                             0.895238\n",
       "val_recall                                           0.414097\n",
       "train_acc                                            0.760128\n",
       "train_f1                                             0.765807\n",
       "train_mcc                                            0.526185\n",
       "train_prec                                           0.751276\n",
       "train_recall                                         0.789661\n",
       "epoch                                                       2\n",
       "step                                                       69\n",
       "timestamp                                          1708710088\n",
       "checkpoint_dir_name                         checkpoint_000002\n",
       "should_checkpoint                                        True\n",
       "done                                                     True\n",
       "training_iteration                                          3\n",
       "trial_id                                          823bd_00002\n",
       "date                                      2024-02-23_17-41-28\n",
       "time_this_iter_s                                    171.78774\n",
       "time_total_s                                       538.451462\n",
       "pid                                                   1319473\n",
       "hostname                                                  rho\n",
       "node_ip                                           10.1.103.91\n",
       "time_since_restore                                 538.451462\n",
       "iterations_since_restore                                    3\n",
       "config/train_loop_config/fully_layer_1                     32\n",
       "config/train_loop_config/fully_layer_2                     16\n",
       "config/train_loop_config/learning_rate                 0.0005\n",
       "config/train_loop_config/batch_size                        64\n",
       "config/train_loop_config/drop_rate                        0.3\n",
       "logdir                                            823bd_00002\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.get_dataframe()\n",
    "result_df.loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__fspath__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_annotated',\n",
       " '_get_temporary_checkpoint_dir',\n",
       " '_uuid',\n",
       " 'as_directory',\n",
       " 'filesystem',\n",
       " 'from_directory',\n",
       " 'get_metadata',\n",
       " 'path',\n",
       " 'set_metadata',\n",
       " 'to_directory',\n",
       " 'update_metadata']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(result.get_best_result(metric=\"val_mcc\",mode=\"max\").checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fully_layer_1': 256,\n",
       " 'fully_layer_2': 32,\n",
       " 'learning_rate': 0.0001,\n",
       " 'batch_size': 32,\n",
       " 'drop_rate': 0.3}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get_best_result(metric=\"val_mcc\",mode=\"max\").config[\"train_loop_config\"][\"batch_size\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjinich/disco/che_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-23 17:32:20,278\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-02-23 17:32:20,441\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-02-23 17:32:22,194\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from engine.hyperparameters_tune_raytune import deepscreen_hyperparameter_tuneing\n",
    "from utils.configurations import configs\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-02-23 17:59:54</td></tr>\n",
       "<tr><td>Running for: </td><td>00:27:28.79        </td></tr>\n",
       "<tr><td>Memory:      </td><td>18.3/503.8 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=8<br>Bracket: Iter 6.000: 0.39697661002477014 | Iter 2.000: 0.3849924504756928<br>Logical resource usage: 2.0/48 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">   train_loop_config/ba\n",
       "tch_size</th><th style=\"text-align: right;\">    train_loop_config/dr\n",
       "op_rate</th><th style=\"text-align: right;\">    train_loop_config/fu\n",
       "lly_layer_1</th><th style=\"text-align: right;\">    train_loop_config/fu\n",
       "lly_layer_2</th><th style=\"text-align: right;\">       train_loop_config/le\n",
       "arning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  val_acc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_823bd_00000</td><td>TERMINATED</td><td>10.1.103.91:1319471</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">512</td><td style=\"text-align: right;\">0.005 </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">        1165.19 </td><td style=\"text-align: right;\">    0.515623</td><td style=\"text-align: right;\"> 11.3551  </td><td style=\"text-align: right;\"> 0.376374</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00001</td><td>TERMINATED</td><td>10.1.103.91:1319472</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        1644.16 </td><td style=\"text-align: right;\">    0.424775</td><td style=\"text-align: right;\">  0.404472</td><td style=\"text-align: right;\"> 0.832418</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00002</td><td>TERMINATED</td><td>10.1.103.91:1319473</td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         538.451</td><td style=\"text-align: right;\">    0.454345</td><td style=\"text-align: right;\">  0.637153</td><td style=\"text-align: right;\"> 0.604396</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00003</td><td>TERMINATED</td><td>10.1.103.91:1319474</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\">512</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         517.126</td><td style=\"text-align: right;\">    0.863634</td><td style=\"text-align: right;\">  1.07077 </td><td style=\"text-align: right;\"> 0.626374</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00004</td><td>TERMINATED</td><td>10.1.103.91:1319475</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">512</td><td style=\"text-align: right;\">0.005 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         510.685</td><td style=\"text-align: right;\">    1.42018 </td><td style=\"text-align: right;\">  0.681402</td><td style=\"text-align: right;\"> 0.541209</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00005</td><td>TERMINATED</td><td>10.1.103.91:1319476</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         503.57 </td><td style=\"text-align: right;\">    0.418373</td><td style=\"text-align: right;\">  1.81249 </td><td style=\"text-align: right;\"> 0.376374</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00006</td><td>TERMINATED</td><td>10.1.103.91:1319479</td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">0.2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">0.005 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         536.264</td><td style=\"text-align: right;\">    0.535115</td><td style=\"text-align: right;\">  1.64094 </td><td style=\"text-align: right;\"> 0.436813</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00007</td><td>TERMINATED</td><td>10.1.103.91:1319478</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.5</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        1570.09 </td><td style=\"text-align: right;\">    0.233452</td><td style=\"text-align: right;\">  0.463577</td><td style=\"text-align: right;\"> 0.815934</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00008</td><td>TERMINATED</td><td>10.1.103.91:1319477</td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">512</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         535.656</td><td style=\"text-align: right;\">    0.419671</td><td style=\"text-align: right;\">  1.89361 </td><td style=\"text-align: right;\"> 0.379121</td></tr>\n",
       "<tr><td>TorchTrainer_823bd_00009</td><td>TERMINATED</td><td>10.1.103.91:1319480</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">0.5</td><td style=\"text-align: right;\">512</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">0.005 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         516.41 </td><td style=\"text-align: right;\">    0.623318</td><td style=\"text-align: right;\">  0.627537</td><td style=\"text-align: right;\"> 0.651099</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=1319480)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=1319480)\u001b[0m - (ip=10.1.103.91, pid=1320314) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 512), ('drop_rate', 0.3), ('learning_rate', 0.005), ('batch_size', 32), ('experiment_result_path', '/big/lab/sjinich/TrypanoDEEPscreen/.experiments/chembl2581_trial')]\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m [rank: 0] Seed set to 123\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m Missing logger folder: /home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00000_0_batch_size=32,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0050_2024-02-23_17-32-25/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m INFO: Using a total of 2270 datapoints\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m INFO: non_random_split datasets splited train=1452,validation=364,test=454\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(RayTrainWorker pid=1320314)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(TorchTrainer pid=1319472)\u001b[0m Started distributed worker processes: \u001b[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(TorchTrainer pid=1319472)\u001b[0m - (ip=10.1.103.91, pid=1320421) world_rank=0, local_rank=0, node_rank=0\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m INFO: Using hyperparameters [('fully_layer_1', 256), ('fully_layer_2', 32), ('drop_rate', 0.3), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '/big/lab/sjinich/TrypanoDEEPscreen/.experiments/chembl2581_trial')]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m GPU available: False, used: False\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m TPU available: False, using: 0 TPU cores\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m IPU available: False, using: 0 IPUs\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m HPU available: False, using: 0 HPUs\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m [rank: 0] Seed set to 123\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Missing logger folder: /home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/lightning_logs\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m INFO: Using a total of 2270 datapoints\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m INFO: non_random_split datasets splited train=1452,validation=364,test=454\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320330)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('val_mcc', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "\u001b[36m(RayTrainWorker pid=1320383)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320314)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00009_9_batch_size=32,drop_rate=0.5000,fully_layer_1=512,fully_layer_2=256,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=1320402)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00008_8_batch_size=64,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000000)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00005_5_batch_size=32,drop_rate=0.2000,fully_layer_1=16,fully_layer_2=32,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('val_mcc', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "\u001b[36m(RayTrainWorker pid=1320314)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00009_9_batch_size=32,drop_rate=0.5000,fully_layer_1=512,fully_layer_2=256,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320402)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00008_8_batch_size=64,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000001)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00004_4_batch_size=32,drop_rate=0.6000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320314)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00009_9_batch_size=32,drop_rate=0.5000,fully_layer_1=512,fully_layer_2=256,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000002)\n",
      "\u001b[36m(RayTrainWorker pid=1320330)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00003_3_batch_size=32,drop_rate=0.3000,fully_layer_1=512,fully_layer_2=16,learning_rate=0.0005_2024-02-23_17-32-25/checkpoint_000002)\n",
      "\u001b[36m(RayTrainWorker pid=1320402)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00008_8_batch_size=64,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000003)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000003)\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00000_0_batch_size=32,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000003)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('train_mcc', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000004)\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('train_mcc', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000004)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000005)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m /home/sjinich/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:211: You called `self.log('train_mcc', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000005)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000006)\n",
      "\u001b[36m(RayTrainWorker pid=1320262)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00000_0_batch_size=32,drop_rate=0.3000,fully_layer_1=128,fully_layer_2=512,learning_rate=0.0050_2024-02-23_17-32-25/checkpoint_000006)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000007)\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000007)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000008)\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000008)\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m `Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "\u001b[36m(RayTrainWorker pid=1320420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00007_7_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=16,learning_rate=0.0010_2024-02-23_17-32-25/checkpoint_000009)\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000009)\n",
      "2024-02-23 17:59:54,143\tINFO tune.py:1042 -- Total run time: 1648.85 seconds (1648.78 seconds for the tuning loop).\n",
      "\u001b[36m(RayTrainWorker pid=1320421)\u001b[0m `Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*Consider increasing the value of the `num_workers` argumen*\")\n",
    "\n",
    "data = pd.read_csv(\"/home/sjinich/disco/TrypanoDEEPscreen/.data/processed/CHEMBL2581.csv\")\n",
    "    \n",
    "search_space_deepscreen = configs.get_hyperparameters_search()\n",
    "\n",
    "tuner = deepscreen_hyperparameter_tuneing(\n",
    "    data=data,\n",
    "    data_split_mode=\"non_random_split\",\n",
    "    search_space=search_space_deepscreen,\n",
    "    target=\"chembl2581_trial\",\n",
    "    experiments_result_path=\"../../.experiments\",\n",
    "    **configs.get_hyperparameters_search_setup()\n",
    "    )\n",
    "\n",
    "result = tuner.tune_deepscreen()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'train_loss': 0.4247746169567108, 'val_loss': 0.4044719636440277, 'val_acc': 0.8324176073074341, 'val_f1': 0.8688172101974487, 'val_mcc': 0.6386048197746277, 'val_prec': 0.848739504814148, 'val_recall': 0.8898678421974182, 'train_acc': 0.8836050629615784, 'train_f1': 0.8851717114448547, 'train_mcc': 0.7681795358657837, 'train_prec': 0.868130624294281, 'train_recall': 0.9106861352920532, 'epoch': 9, 'step': 460},\n",
       "  path='/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25',\n",
       "  filesystem='local',\n",
       "  checkpoint=Checkpoint(filesystem=local, path=/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000009)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(result.get_best_result(metric=\"val_mcc\",mode=\"max\").checkpoint.path,\"checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000009'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msystem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEEPScreenClassifier\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatamodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEEPscreenDataModule\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDEEPScreenClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_mcc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexperiment_result_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../.experiments/chembl2581\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/core/module.py:1552\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1479\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1550\u001b[0m \n\u001b[1;32m   1551\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1552\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/disco/che_env/lib/python3.8/site-packages/lightning/pytorch/core/saving.py:61\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m map_location \u001b[38;5;241m=\u001b[39m map_location \u001b[38;5;129;01mor\u001b[39;00m _default_map_location\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 61\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     64\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     65\u001b[0m     checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m )\n",
      "File \u001b[0;32m~/disco/che_env/lib/python3.8/site-packages/lightning/fabric/utilities/cloud_io.py:54\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[1;32m     51\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     )\n\u001b[1;32m     53\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[0;32m~/disco/che_env/lib/python3.8/site-packages/fsspec/spec.py:1293\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1293\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m~/disco/che_env/lib/python3.8/site-packages/fsspec/implementations/local.py:184\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/disco/che_env/lib/python3.8/site-packages/fsspec/implementations/local.py:306\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/disco/che_env/lib/python3.8/site-packages/fsspec/implementations/local.py:311\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    313\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/sjinich/ray_results/TorchTrainer_2024-02-23_17-32-22/TorchTrainer_823bd_00001_1_batch_size=32,drop_rate=0.3000,fully_layer_1=256,fully_layer_2=32,learning_rate=0.0001_2024-02-23_17-32-25/checkpoint_000009'"
     ]
    }
   ],
   "source": [
    "from engine.system import DEEPScreenClassifier\n",
    "from datasets.datamodule import DEEPscreenDataModule\n",
    "\n",
    "model = DEEPScreenClassifier.load_from_checkpoint(result.get_best_result(metric=\"val_mcc\",mode=\"max\").checkpoint.path,experiment_result_path=\"../../.experiments/chembl2581\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

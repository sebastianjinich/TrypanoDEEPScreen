{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(\"/home/sjinich/disco/TrypanoDEEPscreen/.experiments/raytune/raytune_asha.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_mcc</th>\n",
       "      <th>val_pres</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_mcc</th>\n",
       "      <th>train_pres</th>\n",
       "      <th>...</th>\n",
       "      <th>hostname</th>\n",
       "      <th>node_ip</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>config/train_loop_config/fully_layer_1</th>\n",
       "      <th>config/train_loop_config/fully_layer_2</th>\n",
       "      <th>config/train_loop_config/learning_rate</th>\n",
       "      <th>config/train_loop_config/batch_size</th>\n",
       "      <th>config/train_loop_config/drop_rate</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.357365</td>\n",
       "      <td>0.355337</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.529074</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.881720</td>\n",
       "      <td>0.807245</td>\n",
       "      <td>0.897155</td>\n",
       "      <td>...</td>\n",
       "      <td>c008</td>\n",
       "      <td>10.1.1.8</td>\n",
       "      <td>981.834409</td>\n",
       "      <td>200</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0ee0f_00349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.335850</td>\n",
       "      <td>0.330853</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.496690</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.849167</td>\n",
       "      <td>0.817356</td>\n",
       "      <td>0.691412</td>\n",
       "      <td>0.781853</td>\n",
       "      <td>...</td>\n",
       "      <td>c008</td>\n",
       "      <td>10.1.1.8</td>\n",
       "      <td>923.650962</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0ee0f_00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.350898</td>\n",
       "      <td>0.380187</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.483184</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>0.868167</td>\n",
       "      <td>0.784553</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>...</td>\n",
       "      <td>c008</td>\n",
       "      <td>10.1.1.8</td>\n",
       "      <td>1025.254237</td>\n",
       "      <td>200</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0ee0f_00281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.326714</td>\n",
       "      <td>0.474282</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.460173</td>\n",
       "      <td>0.430769</td>\n",
       "      <td>0.905833</td>\n",
       "      <td>0.880927</td>\n",
       "      <td>0.803062</td>\n",
       "      <td>0.878151</td>\n",
       "      <td>...</td>\n",
       "      <td>c008</td>\n",
       "      <td>10.1.1.8</td>\n",
       "      <td>910.435826</td>\n",
       "      <td>200</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0ee0f_00130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.334552</td>\n",
       "      <td>0.393056</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.456070</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.867804</td>\n",
       "      <td>0.783075</td>\n",
       "      <td>0.875269</td>\n",
       "      <td>...</td>\n",
       "      <td>c008</td>\n",
       "      <td>10.1.1.8</td>\n",
       "      <td>954.335292</td>\n",
       "      <td>200</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0ee0f_00049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.604487</td>\n",
       "      <td>1.156407</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>-0.124550</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.781667</td>\n",
       "      <td>0.730453</td>\n",
       "      <td>0.547753</td>\n",
       "      <td>0.711423</td>\n",
       "      <td>...</td>\n",
       "      <td>c008</td>\n",
       "      <td>10.1.1.8</td>\n",
       "      <td>161.582607</td>\n",
       "      <td>31</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>64</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0ee0f_00298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.537537</td>\n",
       "      <td>1.735415</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>-0.124550</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.711667</td>\n",
       "      <td>0.625541</td>\n",
       "      <td>0.391633</td>\n",
       "      <td>0.640798</td>\n",
       "      <td>...</td>\n",
       "      <td>c008</td>\n",
       "      <td>10.1.1.8</td>\n",
       "      <td>70.004535</td>\n",
       "      <td>11</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0ee0f_00077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.536457</td>\n",
       "      <td>0.984576</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>-0.124550</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.715370</td>\n",
       "      <td>0.504987</td>\n",
       "      <td>0.648881</td>\n",
       "      <td>...</td>\n",
       "      <td>c008</td>\n",
       "      <td>10.1.1.8</td>\n",
       "      <td>160.252098</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0ee0f_00269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.422953</td>\n",
       "      <td>1.700263</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>-0.124550</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.711667</td>\n",
       "      <td>0.628755</td>\n",
       "      <td>0.393266</td>\n",
       "      <td>0.638344</td>\n",
       "      <td>...</td>\n",
       "      <td>c008</td>\n",
       "      <td>10.1.1.8</td>\n",
       "      <td>72.883164</td>\n",
       "      <td>11</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0ee0f_00141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.526992</td>\n",
       "      <td>0.991922</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>-0.124550</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.790833</td>\n",
       "      <td>0.739356</td>\n",
       "      <td>0.565023</td>\n",
       "      <td>0.726531</td>\n",
       "      <td>...</td>\n",
       "      <td>c008</td>\n",
       "      <td>10.1.1.8</td>\n",
       "      <td>164.447945</td>\n",
       "      <td>31</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>64</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0ee0f_00175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train_loss  val_loss   val_acc    val_f1   val_mcc  val_pres  train_acc  \\\n",
       "349    0.357365  0.355337  0.890000  0.592593  0.529074  0.585366   0.908333   \n",
       "13     0.335850  0.330853  0.856667  0.565657  0.496690  0.474576   0.849167   \n",
       "281    0.350898  0.380187  0.876667  0.554217  0.483184  0.534884   0.897500   \n",
       "130    0.326714  0.474282  0.836667  0.533333  0.460173  0.430769   0.905833   \n",
       "49     0.334552  0.393056  0.860000  0.533333  0.456070  0.480000   0.896667   \n",
       "..          ...       ...       ...       ...       ...       ...        ...   \n",
       "298    0.604487  1.156407  0.133333  0.235294 -0.124550  0.133333   0.781667   \n",
       "77     0.537537  1.735415  0.133333  0.235294 -0.124550  0.133333   0.711667   \n",
       "269    0.536457  0.984576  0.133333  0.235294 -0.124550  0.133333   0.750000   \n",
       "141    0.422953  1.700263  0.133333  0.235294 -0.124550  0.133333   0.711667   \n",
       "175    0.526992  0.991922  0.133333  0.235294 -0.124550  0.133333   0.790833   \n",
       "\n",
       "     train_f1  train_mcc  train_pres  ...  hostname   node_ip  \\\n",
       "349  0.881720   0.807245    0.897155  ...      c008  10.1.1.8   \n",
       "13   0.817356   0.691412    0.781853  ...      c008  10.1.1.8   \n",
       "281  0.868167   0.784553    0.880435  ...      c008  10.1.1.8   \n",
       "130  0.880927   0.803062    0.878151  ...      c008  10.1.1.8   \n",
       "49   0.867804   0.783075    0.875269  ...      c008  10.1.1.8   \n",
       "..        ...        ...         ...  ...       ...       ...   \n",
       "298  0.730453   0.547753    0.711423  ...      c008  10.1.1.8   \n",
       "77   0.625541   0.391633    0.640798  ...      c008  10.1.1.8   \n",
       "269  0.715370   0.504987    0.648881  ...      c008  10.1.1.8   \n",
       "141  0.628755   0.393266    0.638344  ...      c008  10.1.1.8   \n",
       "175  0.739356   0.565023    0.726531  ...      c008  10.1.1.8   \n",
       "\n",
       "     time_since_restore iterations_since_restore  \\\n",
       "349          981.834409                      200   \n",
       "13           923.650962                      200   \n",
       "281         1025.254237                      200   \n",
       "130          910.435826                      200   \n",
       "49           954.335292                      200   \n",
       "..                  ...                      ...   \n",
       "298          161.582607                       31   \n",
       "77            70.004535                       11   \n",
       "269          160.252098                       31   \n",
       "141           72.883164                       11   \n",
       "175          164.447945                       31   \n",
       "\n",
       "     config/train_loop_config/fully_layer_1  \\\n",
       "349                                     128   \n",
       "13                                       16   \n",
       "281                                     256   \n",
       "130                                     256   \n",
       "49                                      128   \n",
       "..                                      ...   \n",
       "298                                     256   \n",
       "77                                      512   \n",
       "269                                      16   \n",
       "141                                     128   \n",
       "175                                     256   \n",
       "\n",
       "     config/train_loop_config/fully_layer_2  \\\n",
       "349                                     256   \n",
       "13                                      256   \n",
       "281                                     128   \n",
       "130                                     256   \n",
       "49                                      128   \n",
       "..                                      ...   \n",
       "298                                     256   \n",
       "77                                      128   \n",
       "269                                     128   \n",
       "141                                     512   \n",
       "175                                     512   \n",
       "\n",
       "     config/train_loop_config/learning_rate  \\\n",
       "349                                  0.0001   \n",
       "13                                   0.0001   \n",
       "281                                  0.0005   \n",
       "130                                  0.0001   \n",
       "49                                   0.0001   \n",
       "..                                      ...   \n",
       "298                                  0.0005   \n",
       "77                                   0.0050   \n",
       "269                                  0.0010   \n",
       "141                                  0.0010   \n",
       "175                                  0.0005   \n",
       "\n",
       "    config/train_loop_config/batch_size config/train_loop_config/drop_rate  \\\n",
       "349                                  32                                0.5   \n",
       "13                                   32                                0.6   \n",
       "281                                  32                                0.5   \n",
       "130                                  64                                0.5   \n",
       "49                                   32                                0.5   \n",
       "..                                  ...                                ...   \n",
       "298                                  64                                0.8   \n",
       "77                                   64                                0.5   \n",
       "269                                  64                                0.5   \n",
       "141                                  32                                0.6   \n",
       "175                                  64                                0.8   \n",
       "\n",
       "          logdir  \n",
       "349  0ee0f_00349  \n",
       "13   0ee0f_00013  \n",
       "281  0ee0f_00281  \n",
       "130  0ee0f_00130  \n",
       "49   0ee0f_00049  \n",
       "..           ...  \n",
       "298  0ee0f_00298  \n",
       "77   0ee0f_00077  \n",
       "269  0ee0f_00269  \n",
       "141  0ee0f_00141  \n",
       "175  0ee0f_00175  \n",
       "\n",
       "[350 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(\"val_mcc\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_loss                                           0.357365\n",
       "val_loss                                             0.355337\n",
       "val_acc                                                  0.89\n",
       "val_f1                                               0.592593\n",
       "val_mcc                                              0.529074\n",
       "val_pres                                             0.585366\n",
       "train_acc                                            0.908333\n",
       "train_f1                                              0.88172\n",
       "train_mcc                                            0.807245\n",
       "train_pres                                           0.897155\n",
       "epoch                                                     199\n",
       "step                                                     7600\n",
       "timestamp                                          1708527833\n",
       "checkpoint_dir_name                         checkpoint_000199\n",
       "should_checkpoint                                        True\n",
       "done                                                     True\n",
       "training_iteration                                        200\n",
       "trial_id                                          0ee0f_00349\n",
       "date                                      2024-02-21_12-03-53\n",
       "time_this_iter_s                                     4.594436\n",
       "time_total_s                                       981.834409\n",
       "pid                                                     25311\n",
       "hostname                                                 c008\n",
       "node_ip                                              10.1.1.8\n",
       "time_since_restore                                 981.834409\n",
       "iterations_since_restore                                  200\n",
       "config/train_loop_config/fully_layer_1                    128\n",
       "config/train_loop_config/fully_layer_2                    256\n",
       "config/train_loop_config/learning_rate                 0.0001\n",
       "config/train_loop_config/batch_size                        32\n",
       "config/train_loop_config/drop_rate                        0.5\n",
       "logdir                                            0ee0f_00349\n",
       "Name: 349, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.loc[349,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-27 23:33:47,571\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-02-27 23:33:47,753\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from engine.system import DEEPScreenClassifier\n",
    "from datasets.datamodule import DEEPscreenDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../.data/processed/CHEMBL5567.csv\")\n",
    "datamodule = DEEPscreenDataModule(data=data,target_id=\"CHEMBL2581\",batch_size=32,experiment_result_path=\"../../.experiments/chembl2581\",data_split_mode=\"non_random_split\",tmp_imgs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "def update_and_save_temperature(json_file_path, pickle_file_path,new_temp):\n",
    "    def read_file(file_path, file_type):\n",
    "        with open(file_path, 'rb' if file_type == 'pickle' else 'r') as file:\n",
    "            data = pickle.load(file) if file_type == 'pickle' else json.load(file)\n",
    "        return data\n",
    "\n",
    "    def write_file(file_path, data, file_type):\n",
    "        with open(file_path, 'wb' if file_type == 'pickle' else 'w') as file:\n",
    "            pickle.dump(data, file) if file_type == 'pickle' else json.dump(data, file, indent=2)\n",
    "\n",
    "    # Read JSON file\n",
    "    json_data = read_file(json_file_path, 'json')\n",
    "\n",
    "    # Read Pickle file\n",
    "    pickle_data = read_file(pickle_file_path, 'pickle')\n",
    "\n",
    "    # Add a new key-value pair to each dictionary\n",
    "    json_data[\"train_loop_config\"][\"temperature_scaleing\"] = new_temp\n",
    "    pickle_data[\"train_loop_config\"][\"temperature_scaleing\"] = new_temp\n",
    "\n",
    "    # Write updated data back to JSON file\n",
    "    write_file(json_file_path, json_data, 'json')\n",
    "\n",
    "    # Write updated data back to Pickle file\n",
    "    write_file(pickle_file_path, pickle_data, 'pickle')\n",
    "\n",
    "\n",
    "# Example usage\n",
    "json_file_path = \"/home/sjinich/disco/TrypanoDEEPscreen/.experiments/raytune/TorchTrainer_2024-02-19_21-00-22/TorchTrainer_0ee0f_00349_349_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=256,learning_rate=0.0001_2024-02-21_05-15-56/params.json\"\n",
    "pickle_file_path = \"/home/sjinich/disco/TrypanoDEEPscreen/.experiments/raytune/TorchTrainer_2024-02-19_21-00-22/TorchTrainer_0ee0f_00349_349_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=256,learning_rate=0.0001_2024-02-21_05-15-56/params.pkl\"\n",
    "update_and_save_temperature(json_file_path, pickle_file_path,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 0.8)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning import Trainer\n",
    "import numpy as np\n",
    "\n",
    "model = DEEPScreenClassifier.load_from_checkpoint(\"/home/sjinich/disco/TrypanoDEEPscreen/.experiments/raytune/TorchTrainer_2024-02-19_21-00-22/TorchTrainer_0ee0f_00349_349_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=256,learning_rate=0.0001_2024-02-21_05-15-56/checkpoint_000199/checkpoint.ckpt\",experiment_result_path=\"../../.experiments/chembl2581\",temperature=0.8)\n",
    "\n",
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 1.0)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 1.1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 1.2000000000000002)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 1.3000000000000003)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 1.4000000000000004)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 1.5000000000000004)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 1.6000000000000005)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 1.7000000000000006)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 1.8000000000000007)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 1.9000000000000008)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 2.000000000000001)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 2.100000000000001)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 2.200000000000001)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 2.300000000000001)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 2.4000000000000012)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 2.5000000000000013)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 2.6000000000000014)]\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 2.7000000000000015)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 2.8000000000000016)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 2.9000000000000017)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 3.0000000000000018)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 3.100000000000002)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 3.200000000000002)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 3.300000000000002)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 3.400000000000002)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 3.500000000000002)]\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 3.6000000000000023)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 3.7000000000000024)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 3.8000000000000025)]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using hyperparameters [('fully_layer_1', 128), ('fully_layer_2', 256), ('drop_rate', 0.5), ('learning_rate', 0.0001), ('batch_size', 32), ('experiment_result_path', '../../.experiments/chembl2581'), ('temperature', 3.9000000000000026)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO: Using a total of 1876 datapoints\n",
      "/big/lab/sjinich/che_env/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from lightning import Trainer\n",
    "import numpy as np\n",
    "\n",
    "val_results_df = pd.DataFrame()\n",
    "\n",
    "for temp in np.arange(1,4,0.1):\n",
    "\n",
    "    model = DEEPScreenClassifier.load_from_checkpoint(\"/home/sjinich/disco/TrypanoDEEPscreen/.experiments/raytune/TorchTrainer_2024-02-19_21-00-22/TorchTrainer_0ee0f_00349_349_batch_size=32,drop_rate=0.5000,fully_layer_1=128,fully_layer_2=256,learning_rate=0.0001_2024-02-21_05-15-56/checkpoint_000199/checkpoint.ckpt\",experiment_result_path=\"../../.experiments/chembl2581\",temperature=temp)\n",
    "\n",
    "    trainer = Trainer()\n",
    "\n",
    "    val_results = trainer.validate(model,datamodule=datamodule,verbose=False)\n",
    "\n",
    "    val_results_df = pd.concat([val_results_df,pd.Series(val_results[0],name=temp)],axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
